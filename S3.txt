* Amazon S3 is advertised as the "infinitely" scaling storage

is used for backup and storage - files, discs, db backups
disaster recovery
archive files
hybrid cloud storage
application hosting
media hosting
data lakes and big data analytics
software delivery
static websites

stores files inside buckets
buckets => "directories"
buckets must have globally unique names (Across all regions all account (all account that exist in AWS globally) )
buckets are defined at the region level - specific AWS region
naming conventions for s3 buckets:
    no uppercase, no underscore
    3-63 characters long
    not an IP
    must not start with prefix xn--
    must not end with suffix -s3alias

objects:
files have a key
the key is the full path => s3://my-bucket/my_file.sql/txt

object values are the content of the body:
max file size is 5TB => 5000GB
uploading more than 5GB, must use "multi part upload"
* metadata
* tags

security bucket policy:
user based:
    IAM policy - which api should be allowed for a specific user from IAM
Resource-Based:
     Bucket policies - bucket wide rules from the S3 console - allows cross account
     object access control list - finer grain (can be disabled)
     Bucket access control (at the bucket level) - less common (can be disabled)
Note: an IAM principal can access an S3 object if:
    * the user IAM permissions ALLOWS it OR the resource policy allows it
    * AND there is no explicit DENY in the action
Encryption: encrypt object in Amazon S3 using encryption keys

S3 bucket policies:
* JSON based policies
    *in the json file there is the resource block
    the recourse tell the policy what buckets and objects this policy applies to
    *effect: allow/deny => allow or deny actions
    *actions: set of api to allow or deny

use an S3 bucket policy to:
    *grant public access to the bucket
    *force objects ti be encrypted at upload
    *grant access to another account (cross account)

bucket settings for block public access:
these setting were created to prevent company data leaks
if you know your bucket should never be public, leave these settings on
can be set at the account level


static website hosting
s3 can host static websites and have them accessible on the internet
the website URL will be (depending on the region)
if you get a 403 forbidden error, make sure the bucket policy allows public reads! or it may not be public if you did allow
public reads and therefore you must attach an S3 bucket policy that allows it to be public

versioning:
you can version your files on AWS S3
it is enabled at the bucket level
same key overwrite will change the 'version': 1 , 2 , 3 and etc...
it is best practice to version your buckets because:
    it protects against unintended deletes (Ability to restore a version)
    easy rollback to a previous version
notes:
any file that is not versioned prior to enabling versioning will have version "null"
and if you suspend versioning - it does not delete the previous versions

AWS S3 - replication (CRR && SRR)

CRR => cross region replication
SRR => same region replication

the idea is that we have an s3 bucket in one region and a target s3 bucket in another region
and we want to setup async replication between the two buckets
to do so we first must enable versioning in the source and the destination buckets
the copying happens asynchronously - so the replication mechanism happens behind the scenes
to make the replications work we must give proper IAM permissions to S3 so that it has the permission
to read and write from specified buckets

use cases:
    CRR - compliance, lower latency access, replication across accounts
    SRR - log aggregation, live replication between production and test accounts

S3 storage classes:
Amazon S3 standard - general purpose
Amazon S3 standard-infrequent Access (IA)
Amazon S3 one zone-infrequent access
archives:
Amazon S3 glacier instant retrieval
Amazon S3 glacier deep archive
Amazon S3 intelligent tiering

can move between classes manually or using S3 lifecycle configurations

S3 durability and availability:
* high durability of objects accross multiple AZ called eleven nines => 99.999999999%
* if you store 10 million objects with Amazon S3, you can average expect to incur a loss of a single object once every 10K years
* same for all storage classes

Availability:
* measures ow readily available a service is
* varies depending on storage class
* example: s3 standard has 99.99% availability - not available 53 minutes a year

S3 standard:
99.99% availability
used for frequently accessed data
low latency and high throughput
sustain 2 concurrent facility failures
use cases: big data analytics, mobile and gaming
Standard	Frequently accessed data (more than once a month) with milliseconds access

infrequent access:
for data that is less frequently accessed but requires rapid access when needed
low cost than s3 standard

Intelligent-Tiering -	Data with changing or unknown access patterns

Standard-IA	Infrequently - accessed data (once a month) with milliseconds access => low latency

One Zone-IA	- Recreatable, infrequently accessed data (once a month) stored in a single Availability Zone with milliseconds access => data is lost once AZ is destroyed

Glacier Instant Retrieval	- Long-lived archive data accessed once a quarter with instant retrieval in milliseconds

Glacier Flexible Retrieval - (formerly Glacier)	Long-lived archive data accessed once a year with retrieval of minutes to hours

Glacier Deep Archive	- Long-lived archive data accessed less than once a year with retrieval of hours

Reduced redundancy	- Noncritical, frequently accessed data with milliseconds access (not recommended as S3 Standard is more cost effective)

encryption:
server side => is always on provided by AWS, when you upload a file to AWS S3 bucket the server encrypts the file for security purposes
client side => the user encrypts the file (with a lock) before uploading it to AWS S3

shared responsibility model for s3:
AWS => infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities)
configuration and vulnerability analysis
compliance validation

user => setup correctly S3 versioning
S3 bucket policies
S3 replication setup
logging and monitoring
S3 storage classes
make sure you are using the most optimal cost storage that is going to be the most cost friendly that is also your responsibility
data encryption at rest in transit - your responsibility to encrypt your data when uploading it to Amazon S3

AWS snow family:
high secure, portable devices to collect and process data at the edge or to migrate data into and out of AWS
data migration: 3 different devices => snowcone, snowball edge, snowmobile
edge computing: snowcone snowball  edge

the data shipping occurs physical via the AWS snowball device => order the device and receive it via post, transfer the data onto the device
ship it back to AWS => aws imports the data to their infrastructure and exported to s3 bucket

snowball edge for data transfers:
physical data transport solution van move TB's and PB's in or out of AWS
alternative for moving data over the network
pay per data transfer job
provide block storage and Amazon S3-compatible object storage

snowball edge storage optimized:
gives us 80 TB of HDD capacity for block volumes and S3 compatible object storage

snowball edge compute optimized:
gives us 42 TB of HDD or 28TB NVME for block volume and S3 compatible object storage

use cases: large data cloud migrations, DC decomission, disaster recovery

AWS snowcone && snowcone SSD:
small, portable computing device, rugged and secure => meant for environments where you have little amount of data
it is light => 2.1KG
device used for edge computing, storage and data transfer
two flavors that comes with it:
snowcone - 8 TB of HDD storage
snowcone SSD - 14 TB od SSD storage
use snowcone where snowball does not fir ( space constrained environment )
must provide your own battery / cables

in order to send data back to aws  you have two options:
either send the data back offline by shipping it,
or you can connect this device to the internet and use AWS DataSync to send data back to AWS.

AWS Snowmobile:
is an actual truck that is going to transfer data
transfer exabytes of data (1 EB = 1000 PB = 1000000 TB's)
each snowmobile has 100 PB of capacity (use multiple in parallel)
it's high security: temperature controlled, GPS, 24/7 video surveillance
better than snowball if you transfer more than 10PB

how do we use a snow family device?
1. request snowball devices, from the AWS console for delivery
2. install the snowball client / AWS OpsHub on your servers
3. connect the snowball to your servers and copy files using the client
4. ship back the device when your done (goes to the right AWS facility)
5. the data will be loaded into an S3 bucket
6. the snowball will be completely wiped to the highest security measures
