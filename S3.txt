* Amazon S3 is advertised as the "infinitely" scaling storage

is used for backup and storage - files, discs, db backups
disaster recovery
archive files
hybrid cloud storage
application hosting
media hosting
data lakes and big data analytics
software delivery
static websites

stores files inside buckets
buckets => "directories"
buckets must have globally unique names (Across all regions all account (all account that exist in AWS globally) )
buckets are defined at the region level - specific AWS region
naming conventions for s3 buckets:
    no uppercase, no underscore
    3-63 characters long
    not an IP
    must not start with prefix xn--
    must not end with suffix -s3alias

objects:
files have a key
the key is the full path => s3://my-bucket/my_file.sql/txt

object values are the content of the body:
max file size is 5TB => 5000GB
uploading more than 5GB, must use "multi part upload"
* metadata
* tags

security bucket policy:
user based:
    IAM policy - which api should be allowed for a specific user from IAM
Resource-Based:
     Bucket policies - bucket wide rules from the S3 console - allows cross account
     object access control list - finer grain (can be disabled)
     Bucket access control (at the bucket level) - less common (can be disabled)
Note: an IAM principal can access an S3 object if:
    * the user IAM permissions ALLOWS it OR the resource policy allows it
    * AND there is no explicit DENY in the action
Encryption: encrypt object in Amazon S3 using encryption keys

S3 bucket policies:
* JSON based policies
    *in the json file there is the resource block
    the recourse tell the policy what buckets and objects this policy applies to
    *effect: allow/deny => allow or deny actions
    *actions: set of api to allow or deny

use an S3 bucket policy to:
    *grant public access to the bucket
    *force objects to be encrypted at upload
    *grant access to another account (cross account)

bucket settings for block public access:
these setting were created to prevent company data leaks
if you know your bucket should never be public, leave these settings on
can be set at the account level


static website hosting
s3 can host static websites and have them accessible on the internet
the website URL will be (depending on the region)
if you get a 403 forbidden error, make sure the bucket policy allows public reads! or it may not be public if you did allow
public reads and therefore you must attach an S3 bucket policy that allows it to be public

versioning:
you can version your files on AWS S3
it is enabled at the bucket level
same key overwrite will change the 'version': 1 , 2 , 3 and etc...
it is best practice to version your buckets because:
    it protects against unintended deletes (Ability to restore a version)
    easy rollback to a previous version
notes:
any file that is not versioned prior to enabling versioning will have version "null"
and if you suspend versioning - it does not delete the previous versions

AWS S3 - replication (CRR && SRR)

CRR => cross region replication
SRR => same region replication

the idea is that we have an s3 bucket in one region and a target s3 bucket in another region
and we want to setup async replication between the two buckets
to do so we first must enable versioning in the source and the destination buckets
the copying happens asynchronously - so the replication mechanism happens behind the scenes
to make the replications work we must give proper IAM permissions to S3 so that it has the permission
to read and write from specified buckets

use cases:
    CRR - compliance, lower latency access, replication across accounts
    SRR - log aggregation, live replication between production and test accounts

S3 storage classes:
Amazon S3 standard - general purpose
Amazon S3 standard-infrequent Access (IA)
Amazon S3 one zone-infrequent access
archives:
Amazon S3 glacier instant retrieval
Amazon S3 glacier deep archive
Amazon S3 intelligent tiering

can move between classes manually or using S3 lifecycle configurations

S3 durability and availability:
* high durability of objects accross multiple AZ called eleven nines => 99.999999999%
* if you store 10 million objects with Amazon S3, you can average expect to incur a loss of a single object once every 10K years
* same for all storage classes

Availability:
* measures ow readily available a service is
* varies depending on storage class
* example: s3 standard has 99.99% availability - not available 53 minutes a year

S3 standard:
99.99% availability
used for frequently accessed data
low latency and high throughput
sustain 2 concurrent facility failures
use cases: big data analytics, mobile and gaming
Standard	Frequently accessed data (more than once a month) with milliseconds access

infrequent access:
for data that is less frequently accessed but requires rapid access when needed
low cost than s3 standard

Intelligent-Tiering -	Data with changing or unknown access patterns

Standard-IA	Infrequently - accessed data (once a month) with milliseconds access => low latency

One Zone-IA	- Recreatable, infrequently accessed data (once a month) stored in a single Availability Zone with milliseconds access => data is lost once AZ is destroyed

Glacier Instant Retrieval	- Long-lived archive data accessed once a quarter with instant retrieval in milliseconds

Glacier Flexible Retrieval - (formerly Glacier)	Long-lived archive data accessed once a year with retrieval of minutes to hours

Glacier Deep Archive	- Long-lived archive data accessed less than once a year with retrieval of hours

Reduced redundancy	- Noncritical, frequently accessed data with milliseconds access (not recommended as S3 Standard is more cost effective)

encryption:
server side => is always on provided by AWS, when you upload a file to AWS S3 bucket the server encrypts the file for security purposes
client side => the user encrypts the file (with a lock) before uploading it to AWS S3

shared responsibility model for s3:
AWS => infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities)
configuration and vulnerability analysis
compliance validation

user => setup correctly S3 versioning
S3 bucket policies
S3 replication setup
logging and monitoring
S3 storage classes
make sure you are using the most optimal cost storage that is going to be the most cost friendly that is also your responsibility
data encryption at rest in transit - your responsibility to encrypt your data when uploading it to Amazon S3

AWS snow family:
high secure, portable devices to collect and process data at the edge or to migrate data into and out of AWS
data migration: 3 different devices => snowcone, snowball edge, snowmobile
edge computing: snowcone and snowball edge

the data shipping occurs physical via the AWS snowball device => order the device and receive it via post, transfer the data onto the device
ship it back to AWS => aws imports the data to their infrastructure and exported to s3 bucket

snowball edge for data transfers:
physical data transport solution can move TB's and PB's in or out of AWS
alternative for moving data over the network
pay per data transfer job
provide block storage and Amazon S3-compatible object storage

snowball edge storage optimized:
gives us 80 TB of HDD capacity for block volumes and S3 compatible object storage

snowball edge compute optimized:
gives us 42 TB of HDD or 28TB NVME for block volume and S3 compatible object storage

use cases: large data cloud migrations, DC decomission, disaster recovery

AWS snowcone && snowcone SSD:
small, portable computing device, rugged and secure => meant for environments where you have little amount of data
it is light => 2.1KG
device used for edge computing, storage and data transfer
two flavors that comes with it:
snowcone - 8 TB of HDD storage
snowcone SSD - 14 TB of SSD storage
use snowcone where snowball does not fit ( space constrained environment )
must provide your own battery / cables

in order to send data back to aws  you have two options:
either send the data back offline by shipping it,
or you can connect this device to the internet and use AWS DataSync to send data back to AWS.

AWS Snowmobile:
is an actual truck that is going to transfer data
transfer exabytes of data (1 EB = 1000 PB = 1000000 TB's)
each snowmobile has 100 PB of capacity (use multiple in parallel)
it's high security: temperature controlled, GPS, 24/7 video surveillance
better than snowball if you transfer more than 10PB

how do we use a snow family device?
1. request snowball devices, from the AWS console for delivery
2. install the snowball client / AWS OpsHub on your servers
3. connect the snowball to your servers and copy files using the client
4. ship back the device when your done (goes to the right AWS facility)
5. the data will be loaded into an S3 bucket
6. the snowball will be completely wiped to the highest security measures

what is edge computing:
edge computing is when you process data while it;s being created on an edge location
edge location is anything that does not have an internet or that is far away from the cloud
these locations may have:
limited / no internet access such is in a ship, mining underground and etc...
limited / no access to computing power
but you may still want to process data n these locations and for this we need edge computing
so we setup a snowball edge / snowcone device to do edge computing
use cases of edge computing:
* preprocess data
* machine learning at the edge
* transcoding media streams

eventually (if need be) we can ship back the device to AWS (for transferring data for example)


all snow family edge computing can run EC2 instances and AWS lambda functions ( using AWS IoT Greengrass )
long terms deployment options: 1 and 3 years discounted price

AWS OpsHub:
to use a snow family devices, you needed a CLI
today, you can use AWS OpsHub a software you install on your computer to manage your snow family device
gives us a graphic UI to configure and connect to a snow family device
unlocking and configuring single or clustered devices
transferring files
launching and managing instances running on snow family devices

Hybrid cloud for storage:
AWS is pushing for "Hybrid cloud" => meaning part of the infrastructure is on premises and part is on the cloud
S3 is a proprietary storage technology, so how do you expose the S3 data on premise?
to expose S3 data on premise we have to use something called a storage gateway
AWS Storage gateway:

storage options on AWS:
1. Block => EBS or EC2 instance store
2. File => file storage => network file system = Amazon EFS
3. Object => Amazon S3 or Glacier

the storage gateway is going to be bridging between your on-premises data and cloud data in AWS
it provides a Hybrid storage service to allow on premises to seamlessly use the AWS cloud to extend
the storage capability, this can be used for disaster recovery, backup & restore tiered storage

types of storage gateway:
* file gateway
* volume gateway
* tape gateway

under the hood / behind the scenes:
the storage gateway will be using Amazon EBS, Amazon S3 and glacier

so to sum up - the storage gateway is to bridge your file systems and your storage on premises into the cloud to leverage
best of both worlds. (Hybrid)

snowball family summary:

snowball: device that moves large amounts of data from your place to AWS
comes in two sized , 50TB or 80 Tb meaning it cas store 50TB or 80TB

snowball edge: like snowball but does data processing while transporting/moving the data
comes in two sizes: 42TB of HDD storage with compute capability or 80 TB storage optimized

snowmobile: truck is designed for massive data transfers - can transfer EB of data => 1 EB = 1000 PB = 1000000 TB
it has 100PB of capacity - migrate up to 100PB

snowcone: small portalbe device that provides edge computing to a places that has minimal or no internet access,
provides edge computing and data transfer in remote or challenging environments, it can handle up to 8 TB of data

snowcone SSD: can handle up to 14TB of storage

summary of storage classes:

s3 standard: regular storage high durability and availability with immediete data retrieval milliseconds
intelligent tiering: moves data between classes ( faster or slower) storage based on usage, immediete retreival millisecond in

S3 Standard:

    Frequent Access Tier: Immediate (millisecond latency)
    Infrequent Access Tier: A few milliseconds to seconds

S3 One Zone-IA (Infrequent Access):

    Immediate: A few milliseconds to seconds
    Delayed: A few hours (usually within minutes to hours)

frequent retrieval
and minutes to hours  in infrequent retrieval
one zone AI: for less frequent accessed data, lower redunduncy usally slower than s3 and intelligent tiering

s3 glacier - for archiving data you rarely need: has differnet storage options:
expedited: 1 min to 5 mins
standard => 3 to 5 hours
bulk => 5 to 12 hours

deep dive: cheapest option for long term archive - slower to retrieve
12 to 48 hours






